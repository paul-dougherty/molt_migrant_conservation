####### In this script, I use the daymetr package (https://cran.r-project.org/web/packages/daymetr/vignettes/daymetr-vignette.html) 
####### to download daily precipitation data in daymet tiles that cover 
####### the approximate area of the North American monsoon. I then calculate 
####### mean daily precipitation in each tile for each day from 1980-2019.
#=============================================================================

#setwd("~/Documents/Dissertation Research/Molt Migrant Conservation")
setwd("/Volumes/commons/CarlingLab/pdoughe1/Molt_migrant_conservation/monsoon.prcp/")

#loading the required packages
library(daymetr) # for downloading Daymet data
library(ncdf4) # for netcdf manipulation
library(raster) # for raster manipulation
library(rgdal) # for geospatial analysis
library(ggplot2)
library(tidyverse)
library(sf)

# Preparing Daymet Data ---------------------------------------------------

#years to download Daymet data for
years <- c(1980:2019) #Daymet data goes back to 1980

# In the current version of this script, I select tiles that constitute my 
# region of interest from this map: https://daymet.ornl.gov/static/graphics/Tiles_DaymetV3.png,
# and then download data for those tiles. I think it would be better to download data for tiles 
# that overlap with a given shapefile, but I haven't found one of those 
# for the North American monsoon yet / haven't figured out how to do that

# defining tiles for molting grounds and breeding grounds
monsoon.tiles <- c((11012:11017), (10832:10837), (10653:10658), (10473:10478), (10294:10299), 
                   (10115:10120), (9937:9940), (9758:9761), (9580:9582))
# breeding.tiles <- c((12447:12458), (12268:12278), (12088:12098), (11908:11918), (11728:11738),
#                     (11549:11558), (11369:11378), (11190:11198), (11011:11018))

## Downloading the prcp data for each tile and saving to working directory. 
## This can take a long time if you're downloading data for many tiles and/or 
## many years. I did this in Teton to save time.
for(tile in monsoon.tiles){
  for(year in years){
    download_daymet_tiles(tiles = tile,
                          start = year,
                          end = year,
                          param = "prcp", #specify which variable(s) you want data for
                          path = "/pfs/tsfs1/gscratch/pdoughe1/monsoon.prcp") #if doing in Teton
  }
}



# extracting mean precipitation values for each tile ----------------------

# The above code downloads data for 1 x 1 km pixels in each tile. The following 
# code transforms the .nc files into a dataframe and averages daily precipitation 
# across all pixels in a tile.

# making a vector of the names of all tiles for which you downloaded prcp. data
nc_names <- list.files(pattern = ".nc")
#test_names <- c("prcp_2000_11015.nc", "prcp_2001_11015.nc")

## extracting mean prcp. values for each tile on each day, and saving as a .csv file in the working directory
## also extracting coordinates for each Daymet square

# creating empty lists to hold the data
tile.prcp = vector('list', length(nc_names))
tile.coords = vector('list', length(nc_names)) # also recording coordinates for each tile

for (file in nc_names){
  ##most of this code comes from https://rpubs.com/boyerag/297592
  #reading in the netCDF file contents
  nc_data <- nc_open(file)
  
  #reading in lat, long, and time attributes
  lon <- ncvar_get(nc_data, "lon")
  lat <- ncvar_get(nc_data, "lat", verbose = F)
  t <- ncvar_get(nc_data, "time")

  #reading in data for prcp variable and verifying dimensions
  prcp.array <- ncvar_get(nc_data, "prcp") # store the data in a 3-dimensional array

  #seeing what fill value was for missing data
  fillvalue <- ncatt_get(nc_data, "prcp", "_FillValue")

  #closing the netCDF file because we're all done reading in the data
  nc_close(nc_data)

  ##working with the data
  #replacing fill values with NA's
  prcp.array[prcp.array == fillvalue$value] <- NA

  ##extracting data (finding the average precipitation values across all squares in the entire tile for each day)
  #converting the 3D array of data into a raster brick
  r_brick <- brick(prcp.array, xmn=min(lat), xmx=max(lat), ymn=min(lon), ymx=max(lon),
                   crs=CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0"))

  #r_brick <- flip(t(r_brick), direction='y')
  prcp.df <- rasterToPoints(r_brick)
  prcp.df <- as.data.frame(prcp.df)

  #removing the latitude and longitude columns of this dataframe, calculating the mean across the entire square for each day
  mean.prcp <- apply(prcp.df[,3:367], 2, mean)
  mean.prcp <- as.data.frame(mean.prcp)
  mean.prcp <- mean.prcp[,1]
  yday <- (1:365)
  year <- regmatches(file, regexpr("19..|20..", file))
  tile <- (sub("prcp_...._", "", file))
  tile <- (sub(".nc", "", tile))

  # binding all together in a dataframe
  tile.prcp[[file]] <- data.frame(tile, year, yday, mean.prcp)

  #adding a month column to this dataframe
  #creating date column
  tile.prcp[[file]]$date <- as.Date(paste(tile.prcp[[file]]$year, tile.prcp[[file]]$yday, sep = "-"), "%Y-%j")

  #extracting month data for each date
  tile.prcp[[file]]$month <- as.numeric(format(tile.prcp[[file]]$date, "%m"))

  
  ## creating a separate dataframe with coordinates for every tile
  min.lat <- min(lat)
  max.lat <- max(lat)
  min.lon <- min(lon)
  max.lon <- max(lon)
  
  tile.coords[[file]] <- data.frame(tile, min.lat, max.lat, min.lon, max.lon, year)
}

#binding both lists into their respective dataframes
prcp.dat <- do.call(rbind, tile.prcp)
coords.df <- do.call(rbind, tile.coords)
# for the coordinates dataframe, getting rid of redundant rows (you need only data for each year)
coords.unique <- distinct(coords.df, tile, min.lat, max.lat, min.lon, max.lon)


#saving these dataframes as csvs
write.csv(prcp.df,"/Volumes/commons/CarlingLab/pdoughe1/Molt_migrant_conservation/prcp.df.csv")
write.csv(coords.unique,"/Volumes/commons/CarlingLab/pdoughe1/Molt_migrant_conservation/coords.df.csv")
